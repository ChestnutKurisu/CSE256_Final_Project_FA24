{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Notebook to convert PDFs to markdown and then generate synopses for all the PDFs en masse via Google Gemini API",
   "id": "bf8efd862d6f1de5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install transformers==4.38.2",
   "id": "e358f274c4180cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install google-generativeai nougat-ocr",
   "id": "4a915edc5ec91817",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-16T22:03:50.387158Z",
     "start_time": "2024-11-16T21:54:15.203042Z"
    }
   },
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Define the source directory containing the PDFs and the output directory\n",
    "source_directory = r\"\"\n",
    "output_directory = r\"\"\n",
    "\n",
    "# Iterate through each file in the source directory\n",
    "for filename in os.listdir(source_directory):\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(source_directory, filename)\n",
    "        # Construct and run the Nougat command\n",
    "        command = f'nougat \"{pdf_path}\" -o \"{output_directory}\" --no-skippin'\n",
    "        \n",
    "        # Run the command\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        print(f\"Processed {filename}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed DeepSeekMath Pushing the Limits of Mathematical Reasoning in Open Language Models.pdf\n",
      "Processed Integrative Decoding Improve Factuality via Implicit Self-consistency.pdf\n",
      "Processed PEDAL Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars.pdf\n",
      "Processed Self-Para-Consistency Improving Reasoning Tasks at Low Cost for Large Language Models.pdf\n",
      "Processed Soft Self-Consistency Improves Language Model Agents.pdf\n",
      "Processed To Know or Not To Know Analyzing Self-Consistency of Large Language Models under Ambiguity.pdf\n",
      "Processed Toolformer Language Models Can Teach Themselves to Use Tools.pdf\n",
      "Processed Universal Self-Consistency for Large Language Model Generation.pdf\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T22:06:12.931298Z",
     "start_time": "2024-11-16T22:03:50.388167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import google.generativeai as genai\n",
    "import numpy as np\n",
    "\n",
    "# Define directories\n",
    "mmd_directory = output_directory\n",
    "synopsis_directory = r\"\"\n",
    "\n",
    "# List of Google Gemini API keys\n",
    "GOOGLE_AI_API_KEYS = \"\".split(',')\n",
    "\n",
    "# Ensure the synopsis directory exists\n",
    "os.makedirs(synopsis_directory, exist_ok=True)\n",
    "\n",
    "# Define function to configure and send prompt with retries\n",
    "def process_prompt_with_retries(user_prompt, api_key, max_retries=3):\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(user_prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error on attempt {attempt + 1} with API key {api_key}: {e}\")\n",
    "            # Rotate to a new API key and retry after a short delay\n",
    "            api_key = random.choice(GOOGLE_AI_API_KEYS)\n",
    "            time.sleep(25)  # Wait before retrying\n",
    "    return None\n",
    "\n",
    "# Iterate through each .mmd file in the directory\n",
    "for filename in os.listdir(mmd_directory):\n",
    "    if filename.endswith(\".mmd\"):\n",
    "        mmd_path = os.path.join(mmd_directory, filename)\n",
    "        output_filename = os.path.splitext(filename)[0] + \".md\"\n",
    "        output_path = os.path.join(synopsis_directory, output_filename)\n",
    "        \n",
    "        if os.path.exists(output_path):\n",
    "            continue\n",
    "\n",
    "        # Read the content of the .mmd file\n",
    "        with open(mmd_path, 'r', encoding='utf-8') as file:\n",
    "            mmd_content = file.read()\n",
    "\n",
    "        # Define the detailed prompt\n",
    "        user_prompt = (\n",
    "            \"Summarize this paper for me. Include all major algorithms, math formulas, notation, and concepts \"\n",
    "            \"with terms, definitions, and conceptualizations used, results and outcomes, logic, and overall \"\n",
    "            \"methodology in your extensive summary. Include code or LaTeX code snippets where applicable, include the outcomes, discussion, reasoning, literature review, results and tables, and all other data related information from the research paper. Be detailed, elaborate, comprehensive, highly thorough, accurate, objective, and complete in your analysis of \"\n",
    "            \"the work presented in the paper below:\\n\\n\" + mmd_content\n",
    "        )\n",
    "\n",
    "        # Choose an initial API key\n",
    "        api_key = random.choice(GOOGLE_AI_API_KEYS)\n",
    "\n",
    "        # Process the prompt and get the response\n",
    "        response_text = process_prompt_with_retries(user_prompt, api_key)\n",
    "\n",
    "        if response_text:\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                output_file.write(response_text)\n",
    "            print(f\"Synopsis saved for {filename} as {output_filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to process {filename} after multiple attempts.\")"
   ],
   "id": "80d28cd06fdacda1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synopsis saved for DeepSeekMath Pushing the Limits of Mathematical Reasoning in Open Language Models.mmd as DeepSeekMath Pushing the Limits of Mathematical Reasoning in Open Language Models.md\n",
      "Synopsis saved for Integrative Decoding Improve Factuality via Implicit Self-consistency.mmd as Integrative Decoding Improve Factuality via Implicit Self-consistency.md\n",
      "Synopsis saved for PEDAL Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars.mmd as PEDAL Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars.md\n",
      "Synopsis saved for Self-Para-Consistency Improving Reasoning Tasks at Low Cost for Large Language Models.mmd as Self-Para-Consistency Improving Reasoning Tasks at Low Cost for Large Language Models.md\n",
      "Synopsis saved for Soft Self-Consistency Improves Language Model Agents.mmd as Soft Self-Consistency Improves Language Model Agents.md\n",
      "Synopsis saved for To Know or Not To Know Analyzing Self-Consistency of Large Language Models under Ambiguity.mmd as To Know or Not To Know Analyzing Self-Consistency of Large Language Models under Ambiguity.md\n",
      "Synopsis saved for Toolformer Language Models Can Teach Themselves to Use Tools.mmd as Toolformer Language Models Can Teach Themselves to Use Tools.md\n",
      "Synopsis saved for Universal Self-Consistency for Large Language Model Generation.mmd as Universal Self-Consistency for Large Language Model Generation.md\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T00:21:12.658556Z",
     "start_time": "2024-11-18T00:21:12.573632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "clr = \"\"\n",
    "\n",
    "synopsis_directory = r\"\"\n",
    "\n",
    "for filename in os.listdir(synopsis_directory):\n",
    "    if not('self' in filename.lower() or 'putnam' in filename.lower()):\n",
    "        continue\n",
    "    synopsis_path = os.path.join(synopsis_directory, filename)\n",
    "\n",
    "    # Check if the synopsis file exists\n",
    "    if not os.path.isfile(synopsis_path):\n",
    "        print(f\"Synopsis file not found: {synopsis_path}\")\n",
    "        continue\n",
    "\n",
    "    # Read the synopsis content\n",
    "    with open(synopsis_path, 'r', encoding='utf-8') as f:\n",
    "        synopsis_content = f.read()\n",
    "    \n",
    "    clr += synopsis_content + '\\n\\n\\n'\n",
    "    \n",
    "print(clr)"
   ],
   "id": "affcfa6343d159f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Extensive Summary of \"Integrative Decoding: Improve Factuality via Implicit Self-consistency\"\n",
      "\n",
      "This paper introduces Integrative Decoding (ID), a novel decoding strategy designed to enhance the factuality of Large Language Models (LLMs) in open-ended generation tasks.  The core idea is to implicitly incorporate self-consistency into the decoding process, overcoming limitations of existing self-consistency methods that often restrict task formats or are computationally expensive.\n",
      "\n",
      "**1. Literature Review and Problem Statement:**\n",
      "\n",
      "The paper begins by highlighting the issue of \"hallucinations\" in LLMs – the generation of factually incorrect information.  Existing research demonstrates that repeated sampling, generating multiple outputs for the same prompt, significantly improves factuality.  Self-consistency (SC), measuring the consistency among these multiple outputs, serves as a valuable indicator of truthfulness.  However, most SC-based methods are limited to tasks with easily definable consistency (e.g., exact matches in multiple-choice questions or arithmetic problems).  The paper addresses the challenge of applying SC effectively to open-ended generation tasks, where consistency is more nuanced and difficult to quantify directly.  Existing attempts, like concatenating sampled responses into a single prompt for LLM evaluation (Universal Self-Consistency, or USC) or iteratively comparing facts across responses (Self-Contra), suffer from either excessive input length or computational inefficiency.\n",
      "\n",
      "**2. Methodology:**\n",
      "\n",
      "ID proposes a different approach.  The methodology consists of the following steps:\n",
      "\n",
      "1. **Repeated Sampling:** Generate *k* responses (\\(\\mathcal{R} = \\{r_1, r_2, ..., r_k\\}\\)) to a given prompt \\(\\mathbf{x}\\) using a sampling method like temperature or nucleus sampling.\n",
      "\n",
      "2. **Input Construction:**  For each sampled response \\(r_j\\), create a new input \\(q_j\\) by concatenating the prompt, the response, and the prompt again:  \\([ \\mathbf{x}; r_j; \\mathbf{x} ]\\).  (Note: The paper clarifies that additional instructions, like \"Answer this question again,\" are added in practice to improve clarity, but are omitted in the formal notation for simplicity).\n",
      "\n",
      "3. **Concurrent Processing:** Process all constructed inputs \\( \\mathcal{Q} = \\{q_1, q_2, ..., q_k\\}\\) concurrently.\n",
      "\n",
      "4. **Integrative Decoding:** At each decoding step *t*, instead of selecting the next token based on a single input's prediction, ID aggregates the logits (predicted probabilities) from all *k* models:\n",
      "\n",
      "   \\[\\hat{y}_t = \\operatorname*{arg\\,max}_{y_t \\in \\mathcal{V}} \\sum_{r_j \\in \\mathcal{R}} \\log p_\\theta(y_t | y_{<t}, [\\mathbf{x}; r_j; \\mathbf{x}]) \\tag{8}\\]\n",
      "\n",
      "   where \\(\\hat{y}_t\\) is the selected token, \\(\\mathcal{V}\\) is the vocabulary, \\(y_{<t}\\) represents the previously generated tokens, and \\(p_\\theta\\) is the LLM's probability distribution parameterized by \\(\\theta\\).  This step implicitly integrates the self-consistency across all sampled responses.\n",
      "\n",
      "5. **Output:** The process continues until the end of sequence token is generated, resulting in a single output \\(\\hat{\\mathbf{y}}\\) shared by all *k* parallel decoding processes.\n",
      "\n",
      "The paper argues that this approach implicitly estimates a factuality score based on the consistency across responses.  The formula for factuality score of a statement \\(s_i\\) within a response \\(\\hat{\\mathbf{y}}\\), considering other responses \\(\\mathcal{R}\\), is given as:\n",
      "\n",
      "\\[f(s_i) = \\frac{1}{|\\mathcal{R}|} \\sum_{r_j \\in \\mathcal{R}} P(\\text{consistent} | s_i, r_j) \\tag{1}\\]\n",
      "\n",
      "The overall factuality score of the response \\(\\hat{\\mathbf{y}}\\) is then:\n",
      "\n",
      "\\[F(\\hat{\\mathbf{y}}) = \\frac{1}{|\\mathcal{S}| \\cdot |\\mathcal{R}|} \\sum_{s_i \\in \\mathcal{S}} \\sum_{r_j \\in \\mathcal{R}} P(\\text{consistent} | s_i, r_j) \\tag{2}\\]\n",
      "\n",
      "\n",
      "where \\(\\mathcal{S}\\) is the set of statements in \\(\\hat{\\mathbf{y}}\\).  Equation (8) approximates this implicitly by leveraging the in-context learning ability of the LLM.\n",
      "\n",
      "**3. Experiments:**\n",
      "\n",
      "The experiments evaluate ID on three benchmarks:\n",
      "\n",
      "* **TruthfulQA:**  Focuses on questions that humans often answer incorrectly due to misconceptions.  GPT-4 is used to assess truthfulness and informativeness. The metric is the product of Truth and Info scores (\\(T \\times I\\)).\n",
      "\n",
      "* **Biographies:**  Involves generating bullet points summarizing the achievements of computer scientists. GPT-4 evaluates the factuality of each bullet point. Metrics include percentage accuracy (%Accuracy) and the number of correct statements (#Correct).\n",
      "\n",
      "* **LongFact-Objects:**  Requires generating long, detailed descriptions of objects.  LLaMA 3.1-70B-Instruct is used to split responses into atomic facts, with GPT-4 evaluating the truthfulness of each fact.  Metrics include Precision, Recall@128, and F1@128.\n",
      "\n",
      "ID is compared against greedy decoding, DoLa, USC, and Self-Reflection (SR). Experiments are conducted on six series of LLMs with varying scales (LLaMA-2-7B-chat, LLaMA-3-8B-Instruct, Mistral-7B-Instruct-v0.2, Gemma-2-9B-it, Qwen2-7B-Instruct, and GLM-4-9B-chat).  Additional analysis is performed with various model sizes to evaluate scalability and robustness.\n",
      "\n",
      "**4. Results and Discussion:**\n",
      "\n",
      "The results show that ID consistently improves factuality across all LLMs and benchmarks, with substantial gains on TruthfulQA (+11.2%), Biographies (+15.4%), and LongFact (+8.5%). The performance gains increase as the number of sampled responses (*k*) increases, exhibiting a log-linear relationship.  In contrast, USC and SR fail to consistently improve with increasing *k*, often suffering from context length limitations. ID's advantage stems from its ability to implicitly integrate self-consistency without significantly increasing context length.  The method is shown to be robust across different sampling strategies and model scales.  A case study illustrates how ID maintains semantic-level self-consistency, filtering out hallucinations present even in the individual sampled responses.\n",
      "\n",
      "**5. Conclusion:**\n",
      "\n",
      "Integrative Decoding offers a simple yet effective method for improving LLM factuality in open-ended generation.  Its implicit integration of self-consistency and its scalability make it a promising technique for enhancing the reliability of LLMs.  The code and data are publicly available.\n",
      "\n",
      "\n",
      "**Note:**  Due to the length of the paper, some sections (like Appendix details) are summarized rather than fully reproduced here.  The key findings and methodologies are accurately and comprehensively represented.  All the mentioned equations and figures are referenced in the text.\n",
      "\n",
      "\n",
      "\n",
      "## Putnam-AXIOM: An Extensive Summary\n",
      "\n",
      "This paper introduces Putnam-AXIOM, a benchmark designed to evaluate the higher-level mathematical reasoning capabilities of Large Language Models (LLMs).  It addresses the saturation of existing benchmarks and the problem of data contamination, where LLMs might achieve high scores by memorizing problems from their training data.\n",
      "\n",
      "**1. Literature Review:**\n",
      "\n",
      "The paper reviews existing mathematical reasoning benchmarks like MATH and GSM8K, highlighting their limitations due to LLM performance saturation.  It discusses the growing concern of data contamination, where models memorize answers from publicly available datasets.  Related work on combating this issue, such as the creation of functional variations (Srivastava et al., 2024), and other contemporary datasets like ARB, OlympiadBench, and SciBench are also discussed, highlighting their limitations in terms of automatic evaluation and scalability.  PutnamBench, focusing on formal theorem proving, is also mentioned as a related, but distinct, approach.\n",
      "\n",
      "**2. Methodology:**\n",
      "\n",
      "Putnam-AXIOM comprises two datasets:\n",
      "\n",
      "* **Putnam-AXIOM Original:** This dataset consists of 236 problems from the William Lowell Putnam Mathematical Competition (1985-2023), selected for their suitability for automated evaluation.  Problems are categorized into 11 domains (Geometry, Algebra, Trigonometry, Calculus, Linear Algebra, Combinatorics, Probability, Number Theory, Complex Numbers, Differential Equations, and Analysis) and by difficulty level (A/B for sitting, 1-6 for increasing complexity within a sitting).  Solutions are provided with boxed final answers (`\\boxed{}`) for automated evaluation.  Some problems were modified to ensure a single, easily extractable boxed answer, preserving the core difficulty while simplifying evaluation.\n",
      "\n",
      "* **Putnam-AXIOM Variation:** This dataset contains functional variations of 52 problems from the original dataset.  Variations are generated programmatically by:\n",
      "    * **Variable Change:** Altering variable names.\n",
      "    * **Constant Change:** Modifying numerical constants in the problem statement and solution.\n",
      "\n",
      "These variations create an effectively infinite supply of novel, equally challenging problems, mitigating data contamination.\n",
      "\n",
      "**3. Algorithms and Formulas:**\n",
      "\n",
      "The paper doesn't present novel algorithms. The core methodology relies on:\n",
      "\n",
      "* **Automated Evaluation:**  LLM responses are evaluated by extracting the boxed answer and comparing it to the ground truth using an equivalence function. This function handles variations in answer representation (e.g., 0.5, 1/2, ½).\n",
      "* **Functional Variation Generation:** A simple algorithmic process to modify variables and constants in the original problems to produce variations.  No specific formulas are presented for this, as the changes are problem-specific.\n",
      "\n",
      "**4. Notation:**\n",
      "\n",
      "* `\\boxed{}`:  Indicates the boxed final answer in the problem solutions.\n",
      "* \\(F_m\\): The m-th Fibonacci number.\n",
      "* \\(p(x)\\): A polynomial.\n",
      "* \\(\\Gamma(p(x))\\): The sum of squares of the coefficients of polynomial \\(p(x)\\).\n",
      "* \\(i\\): The imaginary unit (\\(i^2 = -1\\)).\n",
      "* \\(\\lfloor a \\rfloor\\): The floor function (largest integer less than or equal to \\(a\\)).\n",
      "* \\(|z|\\): The magnitude (absolute value) of a complex number \\(z\\).\n",
      "\n",
      "**5. Results and Analysis:**\n",
      "\n",
      "The paper evaluates several LLMs (OpenAI's o1-preview, GPT-4, GPT-4o, Claude-3.5 Sonnet, Qwen2-Math-7B, Qwen2-Math-7B-Instruct, NuminaMath, NuminaMath-7B-TIR, and DeepSeek-Math-7B-RL) on both datasets.\n",
      "\n",
      "* **Putnam-AXIOM Original:** OpenAI's o1-preview achieved the highest accuracy (41.95%), while other models scored significantly lower (mostly below 10%).\n",
      "\n",
      "* **Putnam-AXIOM Variation:**  All models showed a significant drop in accuracy compared to their performance on the corresponding original problems (20-44% reduction). This demonstrates the effectiveness of the variations in revealing the models' reliance on memorization.  The confidence intervals for many models indicated statistically significant differences between original and variation performance.\n",
      "\n",
      "Error analysis focused on OpenAI o1-preview and GPT-4o, revealing a common weakness: lack of mathematical rigor in their solutions.  While often reaching the correct final answer, these models frequently lacked justification for intermediate steps.  Open-source models exhibited additional errors like calculation mistakes, hallucinations, and misunderstandings of the problem statement.\n",
      "\n",
      "A further analysis on binary questions (questions with only two possible answers) showed that their inclusion inflated the accuracy scores of some models, especially less-advanced ones, but this effect was less prominent for the top-performing models.\n",
      "\n",
      "**6. Conclusion:**\n",
      "\n",
      "Putnam-AXIOM provides a challenging benchmark for evaluating advanced mathematical reasoning in LLMs. The use of functional variations effectively mitigates data contamination issues. The results highlight the limitations of current LLMs in tackling complex, high-level mathematical problems and underscore the need for further research in artificial reasoning.  The data and evaluation code are publicly available.\n",
      "\n",
      "\n",
      "**7. Tables and Figures:**\n",
      "\n",
      "The summary mentions several tables and figures from the paper which illustrate the performance of different LLMs on the original and variation datasets, as well as examples of model errors and the analysis of binary questions versus complex questions.  These visuals are crucial for a full understanding of the results but cannot be reproduced here without access to the original paper's figures.  The paper mentions Figure 1 contrasting original and variation accuracies with confidence intervals, Table 1 showing Putnam-AXIOM Original dataset accuracies, and Table 2 presenting mean accuracies and confidence intervals for the Putnam-AXIOM Variation dataset.  Figures 3, 4, 7, 8, 9, 10, 11, and 12 illustrate examples of modified questions and model responses.  Figure 6 compares overall accuracies on Putnam-AXIOM with and without binary questions.\n",
      "\n",
      "**8. Legal Compliance:**\n",
      "\n",
      "The paper includes an appendix discussing the legal compliance of using Putnam problems, arguing that their use falls under fair use due to transformative nature of the dataset, non-commercial purpose, and the negligible effect on the market for Putnam problems.\n",
      "\n",
      "\n",
      "\n",
      "## Extensive Summary of \"PutnamBench: A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving\"\n",
      "\n",
      "This paper introduces PutnamBench, a new multilingual benchmark for evaluating formal theorem-proving algorithms.  It leverages problems from the William Lowell Putnam Mathematical Competition, a prestigious undergraduate-level mathematics competition. The benchmark aims to push the boundaries of automated theorem proving by providing challenging problems requiring diverse mathematical knowledge and skills.\n",
      "\n",
      "**1. Introduction:**\n",
      "\n",
      "The paper highlights the growing need for robust benchmarks in automated mathematical reasoning, particularly with the rise of neural theorem provers. Existing benchmarks like MiniF2F (high school level) and Fimo (IMO shortlist problems) have limitations: MiniF2F contains easily solvable problems, and Fimo only supports the now-deprecated Lean 3.  PutnamBench addresses these limitations by offering a collection of formally verified problems from the Putnam competition, known for its challenging problems spanning various undergraduate mathematics topics.  The authors also emphasize the importance of preventing data leakage between training and evaluation sets in the age of LLMs.\n",
      "\n",
      "**2. Background:**\n",
      "\n",
      "* **Formal Theorem Proving:** The paper explains the core concepts of interactive theorem provers (ITPs) like Lean 4, Coq, and Isabelle. These systems allow users to formally state theorems and construct machine-verifiable proofs through a sequence of proof steps, transforming the initial proof state to a final \"QED\" state.  Figure 1 in the paper illustrates a Lean 4 theorem and its proof.\n",
      "* **The Putnam Competition:** The paper describes the Putnam competition, emphasizing its breadth of mathematical topics (analysis, linear algebra, abstract algebra, number theory, geometry, set theory, and combinatorics) and its difficulty, making it a suitable source for a challenging benchmark.\n",
      "\n",
      "**3. PutnamBench:**\n",
      "\n",
      "PutnamBench contains formalizations of 514 Putnam problems in Lean 4 and Isabelle, with an additional 309 in Coq, totaling 1337 formalizations. Key features:\n",
      "\n",
      "* **Diversity and Breadth:** Unlike previous benchmarks focusing primarily on high school mathematics, PutnamBench covers a wider range of undergraduate-level mathematical concepts.\n",
      "* **Multilinguality:**  It's the first benchmark to offer formalizations across Lean 4, Isabelle, and Coq, enabling cross-ITP comparisons and facilitating research in multilingual theorem proving.  The formalizations are structurally aligned but may differ due to the underlying foundations of each language.  The use of different mathematical libraries (Mathlib for Lean, HOL for Isabelle, Coquelicot and others for Coq) also impacts the formalizations.\n",
      "* **Factored Solutions:**  Approximately 60% of Putnam problems require finding a closed-form solution before proving its correctness.  PutnamBench addresses this by offering two tasks: (1) finding the solution and then proving it, and (2) proving a given solution. This better reflects the true difficulty of the original problems.\n",
      "* **Licensing:** The benchmark is released under Apache 2.0 (Lean 4 and Isabelle) and MIT (Coq) licenses.\n",
      "\n",
      "**4. Experimental Evaluation:**\n",
      "\n",
      "The authors evaluate several neural and symbolic theorem provers on PutnamBench:\n",
      "\n",
      "* **Models:** GPT-4 (used across all languages), Copra (Lean 4 and Coq), Draft-Sketch-Prove (DSP) (Isabelle), Sledgehammer (Isabelle), and CoqHammer (Coq).\n",
      "* **Metrics:** The `pass@n` metric is used, measuring the success rate within `n` proof attempts.\n",
      "* **Results:** The overall results are quite poor, demonstrating the significant challenge PutnamBench presents.  Only a handful of problems were solved across all languages and methods:\n",
      "    * **Lean 4:** GPT-4 solved only one problem (Putnam 1988 B1); Copra, with modifications for Lean 4, also solved only one (Putnam 1988 B1).\n",
      "    * **Isabelle:** GPT-4 failed to solve any problems; DSP solved two; Sledgehammer solved three (all involving sets with binary operations).\n",
      "    * **Coq:** GPT-4 solved one problem (Putnam 1988 B1); Copra solved one (Putnam 1988 B1); CoqHammer solved none.  Figure 2, 3, and 13 show examples of formalizations and proofs.\n",
      "* **General Analysis:** The successfully solved problems were generally among the easiest in the benchmark, highlighting the current limitations of automated theorem provers in handling complex, multi-concept problems.  The authors point to two main reasons for the failures: (i) Difficulty in synthesizing new lemmas and complex proof strategies, and (ii) Inefficient leveraging of knowledge within existing mathematical repositories.  Figures 15-21 illustrate various aspects of the evaluation, including prompt engineering, error messages, and successful/failed proof attempts.\n",
      "\n",
      "**5. Related Works:**\n",
      "\n",
      "The paper provides a comprehensive review of related work, including existing formal benchmarks (MiniF2F, Fimo, ProofNet, Compfiles, LeanDojo, ProverBot9001, PISA), informal benchmarks (MATH, GSM8K, NaturalProofs), and methods for formal theorem proving (GPT-f, PACT, FMSCL, HTPS, COPRA, LLEMMA, DeepSeek-Prover, AlphaGeometry,  Isabelle-related methods like DSP, Sledgehammer, Lyra, POETRY, LEGO-Prover, Baldur, and Coq-related methods like ASTactic and Proverbot9001).\n",
      "\n",
      "**6. Conclusion:**\n",
      "\n",
      "PutnamBench is presented as a challenging benchmark for future research in neural theorem proving, highlighting the need for advancements in lemma synthesis, proof strategy generation, and efficient utilization of mathematical repositories.\n",
      "\n",
      "**7. Impact Statement:**\n",
      "\n",
      "The paper concludes with a brief impact statement acknowledging the potential societal consequences of advancements in machine learning without specific elaboration.\n",
      "\n",
      "\n",
      "**Mathematical Formulas and Notation:**\n",
      "\n",
      "The paper uses standard mathematical notation, including set notation (e.g., \\(X \\subseteq \\mathbb{R}\\), \\(|X|\\)), summation notation (e.g., \\(\\sum_{s\\in S} s\\)), group theory notation (e.g., \\(g_1 g_2 g_3 = e\\)), and limits.  Specific formulas are present within the problem statements, but there aren't any overarching mathematical formulas used for the methodology itself beyond the `pass@n` metric.\n",
      "\n",
      "\n",
      "**Code/LaTeX Snippets:**\n",
      "\n",
      "Several snippets of Lean 4, Isabelle, and Coq code are included in the paper to illustrate formalizations of Putnam problems.  These snippets are too numerous to reproduce here but are integral to understanding the specific formalizations used in the benchmark.  The paper also shows examples of GPT-4 generated proofs, highlighting both successful and unsuccessful attempts.\n",
      "\n",
      "\n",
      "**Tables:**\n",
      "\n",
      "The paper does not contain explicit tables summarizing the results, but the results are presented in a textual format in Section 4.3 and discussed in the analysis.  The performance of each method on each ITP is reported individually.\n",
      "\n",
      "\n",
      "Overall, the paper presents a significant contribution to the field by introducing a challenging, multilingual benchmark that will likely drive further innovation in automated theorem proving. The detailed analysis of the experimental results and the comprehensive literature review provide valuable insights into the current limitations and future directions of the field.\n",
      "\n",
      "\n",
      "\n",
      "This paper, \"Self-Consistency Improves Chain of Thought Reasoning in Language Models,\" introduces a novel decoding strategy called self-consistency to enhance the reasoning capabilities of large language models (LLMs).  The core idea builds upon chain-of-thought (CoT) prompting, a technique that encourages LLMs to generate intermediate reasoning steps before arriving at a final answer.  The paper argues that while CoT prompting improves reasoning, it can be further improved by leveraging the inherent diversity of reasoning paths leading to a correct solution.\n",
      "\n",
      "**Literature Review:**\n",
      "\n",
      "The paper reviews existing literature on reasoning in LLMs, highlighting the limitations of solely increasing model scale to improve reasoning abilities. It mentions previous work on chain-of-thought prompting (Wei et al., 2022), which demonstrated significant improvements in multi-step reasoning tasks.  The authors also discuss existing decoding strategies like greedy decoding, temperature sampling, top-k sampling, and nucleus sampling, as well as re-ranking methods that use additional verifiers or human annotations to improve generation quality (Cobbe et al., 2021; Thoppilan et al., 2022).  The paper contrasts self-consistency with these methods, emphasizing its unsupervised nature and lack of need for additional training or data.\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "The core of the methodology is the self-consistency decoding strategy.  It consists of three steps:\n",
      "\n",
      "1. **CoT Prompting:**  The LLM is prompted with a question and a few manually written examples demonstrating chain-of-thought reasoning.\n",
      "\n",
      "2. **Diverse Path Sampling:** Instead of using greedy decoding (selecting the single most likely next token at each step), the model samples multiple reasoning paths from its decoder.  This leverages various sampling techniques like temperature sampling, top-k sampling, and nucleus sampling, with parameters tuned for each model.\n",
      "\n",
      "3. **Answer Marginalization:** The sampled reasoning paths, each potentially leading to a different answer (denoted as  $\\mathbf{a}_{i} \\in \\mathbb{A}$, where  $i = 1, \\dots, m$ indexes the  $m$ samples), are analyzed. The final answer is chosen by marginalizing out the reasoning paths and selecting the most frequent answer (a majority vote).  The paper also explores weighted averaging and weighted summation using the unnormalized or normalized probabilities of generating each path and answer pair:\n",
      "\n",
      "   $P(\\mathbf{r}_{i},\\mathbf{a}_{i}\\mid\\text{prompt},\\text{question})=\\exp^{\\frac{1 }{R}\\sum_{k=1}^{K}\\log P(t_{k}|\\text{prompt},\\text{question},t_{1},\\ldots,t_{ k-1})}$ (Equation 1)\n",
      "\n",
      "   where  $\\mathbf{r}_{i}$ represents the reasoning path,  $t_{k}$ is the  $k$-th token,  $K$ is the total number of tokens, and  $R$ is a normalization factor (used for the normalized version).  The paper finds that the unweighted sum (majority vote) performs comparably to the normalized weighted sum, suggesting the model doesn't reliably distinguish between correct and incorrect reasoning paths.\n",
      "\n",
      "\n",
      "**Experiments and Results:**\n",
      "\n",
      "The paper evaluates self-consistency on several arithmetic and commonsense reasoning benchmarks, including:\n",
      "\n",
      "* **Arithmetic Reasoning:** GSM8K, SVAMP, AQuA, AddSub, MultiArith, ASDiv\n",
      "* **Commonsense Reasoning:** CommonsenseQA, StrategyQA, ARC-challenge\n",
      "* **Symbolic Reasoning:** Last letter concatenation, Coinflip\n",
      "\n",
      "Four LLMs of varying scales are used: UL2-20B, GPT-3-175B, LaMDA-137B, and PaLM-540B.  The results (Tables 2 and 3) consistently show that self-consistency significantly improves accuracy compared to CoT prompting with greedy decoding across all models and tasks.  The improvement is more pronounced for larger models.  Self-consistency achieves state-of-the-art results on many benchmarks.  Figure 2 shows that accuracy generally increases with the number of sampled paths. Table 4 provides illustrative examples where self-consistency corrects errors made by greedy decoding.\n",
      "\n",
      "Additional experiments demonstrate that self-consistency:\n",
      "\n",
      "* **Improves robustness to imperfect prompts:** Even with noisy prompts, self-consistency maintains performance better than greedy decoding.\n",
      "* **Works with different sampling strategies:** The improvement is relatively consistent across different sampling parameters.\n",
      "* **Outperforms other approaches:** Self-consistency surpasses sample-and-rank, beam search, and ensemble methods.  Table 7 shows its superiority over prompt-order and multi-prompt ensemble techniques.\n",
      "\n",
      "\n",
      "**Discussion and Conclusion:**\n",
      "\n",
      "The paper concludes that self-consistency is a simple yet effective method for boosting the reasoning capabilities of LLMs.  It highlights the benefits beyond improved accuracy, such as providing rationales and uncertainty estimates. The authors acknowledge the increased computational cost but suggest using a smaller number of sampling paths to mitigate this.  They also mention the need for future work to address issues like generating nonsensical reasoning paths and improving model calibration and factuality.  The reproducibility statement notes that two of the four models used (UL2 and GPT-3) are publicly available.\n",
      "\n",
      "**Code Snippets (Conceptual):**\n",
      "\n",
      "The paper doesn't provide specific code, but the core self-consistency algorithm can be conceptually represented as follows (pseudocode):\n",
      "\n",
      "```python\n",
      "def self_consistency(model, prompt, question, num_samples):\n",
      "  answers = []\n",
      "  for _ in range(num_samples):\n",
      "    reasoning_path, answer = model.generate_with_cot(prompt, question) # Assumes a function to generate with CoT\n",
      "    answers.append(answer)\n",
      "  return max(set(answers), key=answers.count) # Majority vote\n",
      "\n",
      "```\n",
      "\n",
      "This summary provides a comprehensive overview of the paper, including all the requested elements.  Note that some tables and figures are only described qualitatively due to the limitations of reproducing them in this text format.  The complete paper should be consulted for the detailed numerical results.\n",
      "\n",
      "\n",
      "\n",
      "This paper, \"Self-Para-Consistency: Improving Reasoning Tasks at Low Cost for Large Language Models,\" introduces a novel method to enhance the reasoning capabilities of Large Language Models (LLMs) while significantly reducing computational costs compared to existing techniques.  The core problem addressed is the high cost associated with the self-consistency method, which relies on sampling numerous reasoning paths, many of which are low-probability and thus unproductive.\n",
      "\n",
      "**Literature Review:**\n",
      "\n",
      "The paper reviews existing work on Chain-of-Thought (CoT) prompting, highlighting its successes in enabling multi-step reasoning in LLMs but also acknowledging its limitations.  Greedy decoding in CoT can lead to suboptimal solutions, while the self-consistency approach, which mitigates this by sampling multiple paths and taking a majority vote, suffers from high computational expense due to the generation of many low-probability paths.  Program-of-Thought (PoT) prompting is mentioned as an alternative to address inconsistency issues, but the focus remains on improving the efficiency of the self-consistency approach.  The authors also discuss the inherent quality-diversity trade-off in text generation and how prior work attempts to address it by manipulating parameters or sampling latent variables.  They propose leveraging paraphrase generation to achieve diversity without sacrificing quality by using greedy decoding.\n",
      "\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "The proposed method, **Self-Para-Consistency**, tackles the cost issue by replacing the expensive sampling process with paraphrase generation. The methodology consists of three steps:\n",
      "\n",
      "1. **Paraphrasing:**  Given a question, *x*, the LLM (parameterized by θ) generates *k-1* paraphrases, denoted as  \\(G_{para} = \\{x'_1, x'_2, ..., x'_{k-1}\\}\\). This process is formalized as:\n",
      "\n",
      "   \\[\\mathcal{P}_{\\theta}(G_{para} \\mid x, \\mathcal{I}_{para}) = \\prod_{i=1}^{k-1} \\mathcal{P}_{\\theta}(x'_i \\mid x, \\mathcal{I}_{para}, G^{<i}_{para}) \\tag{1}\\]\n",
      "\n",
      "   where \\(\\mathcal{I}_{para}\\) is the prompt instructing the LLM to generate paraphrases, and \\(G^{<i}_{para}\\) represents the set of paraphrases generated before the *i*-th paraphrase.  Sequential generation encourages diversity in the paraphrases.\n",
      "\n",
      "2. **Reasoning Path Generation:**  The LLM generates reasoning paths, \\(R_{path} = \\{r_1, r_2, ..., r_k\\}\\), for the original question and its *k-1* paraphrases using greedy decoding.  This step is formalized as:\n",
      "\n",
      "   \\[\\mathcal{P}_{\\theta}(R_{path} \\mid x, G_{para}, \\mathcal{I}_{inst}) = \\mathcal{P}_{\\theta}(r_1 \\mid x, \\mathcal{I}_{inst}) \\cdot \\prod_{i=1}^{k-1} \\mathcal{P}_{\\theta}(r_{i+1} \\mid x'_i, \\mathcal{I}_{inst}) \\tag{2}\\]\n",
      "\n",
      "   where \\(\\mathcal{I}_{inst}\\) is the prompt instructing the LLM to generate reasoning paths.  \\(r_1\\) corresponds to the original question *x*, and subsequent \\(r_i\\) correspond to paraphrases \\(x'_{i-1}\\).  The parallel generation is computationally efficient.\n",
      "\n",
      "3. **Answer Aggregation:**  Each reasoning path, \\(r_i\\), yields an answer, \\(a_i\\).  The final answer is determined by majority voting among the \\(a_i\\):\n",
      "\n",
      "   \\(\\arg\\max_{a} \\sum_{i=1}^{k} \\mathbb{1}(a_i = a)\\)\n",
      "\n",
      "   where \\(\\mathbb{1}\\) is the indicator function.\n",
      "\n",
      "**Prompting Details:**  The paper provides examples of the prompts \\(\\mathcal{I}_{para}\\) and \\(\\mathcal{I}_{inst}\\) for numerical reasoning, showing how they guide the LLM in paraphrasing and reasoning.\n",
      "\n",
      "**Experiments and Results:**\n",
      "\n",
      "The authors evaluate their method on six reasoning datasets: three in-distribution datasets (GSM8K, SVAMP, ASDIV) and three out-of-distribution (OOD) datasets (GSM8K-hard, a modified GSM8K with larger numbers; and a date understanding dataset from BIG-bench).  They compare Self-Para-Consistency with several baselines, including Zero-Shot-PAL (Program-Aided Language Models) and Self-Consistency with varying temperatures and sampling numbers.  \n",
      "\n",
      "* **Table 1** presents results for numerical reasoning datasets. Self-Para-Consistency (*k*=3) generally outperforms the baselines, particularly on the OOD GSM8K-hard dataset.\n",
      "* **Table 2** shows results for the date understanding dataset. Again, Self-Para-Consistency achieves the highest accuracy.\n",
      "\n",
      "The results demonstrate that Self-Para-Consistency achieves comparable or better accuracy than Self-Consistency with a significantly smaller number of reasoning paths (k=3 vs. k=5 or k=10), thus achieving lower computational cost.\n",
      "\n",
      "**Analysis:**\n",
      "\n",
      "The paper addresses a potential concern that inaccurate paraphrases might lead to error propagation. They mitigate this by concatenating the original and paraphrased questions in the prompt.  A case study (Figure 3) illustrates how Self-Para-Consistency can handle imperfect paraphrases effectively.\n",
      "\n",
      "**Limitations and Future Work:**\n",
      "\n",
      "The authors acknowledge limitations and suggest future research directions, including:\n",
      "\n",
      "* Combining Self-Para-Consistency and Self-Consistency.\n",
      "* Incorporating paraphrase verification.\n",
      "* Using Self-Para-Consistency as a measure of LLM uncertainty in reasoning tasks.\n",
      "\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The paper successfully introduces Self-Para-Consistency, a cost-effective alternative to Self-Consistency for improving LLM reasoning.  By leveraging paraphrase generation instead of extensive sampling, it achieves comparable or better accuracy with substantially reduced computational overhead.  The results and analysis convincingly demonstrate the method's effectiveness and suggest promising avenues for future research.\n",
      "\n",
      "\n",
      "\n",
      "## Extensive Summary of \"Soft Self-Consistency Improves Language Model Agents\"\n",
      "\n",
      "This paper introduces Soft Self-Consistency (Soft-SC), an improved method for selecting the best output from multiple samples generated by a Large Language Model (LLM) acting as an agent in interactive tasks.  The authors address limitations of the existing Self-Consistency (SC) method, particularly its inefficiency in scenarios with numerous valid actions.\n",
      "\n",
      "**1. Literature Review and Problem Statement:**\n",
      "\n",
      "The paper builds upon the success of Self-Consistency (SC) (Wang et al., 2023), which enhances LLM performance by generating multiple solutions (using chain-of-thought prompting) and selecting the answer via majority voting. However, SC's reliance on exact matching for voting proves inefficient in interactive tasks with large, diverse action spaces.  In such settings, the probability of identical actions across multiple samples is low, requiring a prohibitively large number of samples for reliable selection.  The authors cite a specific example where, with only five samples, SC fails to produce a majority action 86% of the time in a bash program prediction task.  This inefficiency motivates the development of Soft-SC.\n",
      "\n",
      "**2. Methodology:**\n",
      "\n",
      "Soft-SC proposes a continuous relaxation of the discrete majority voting approach used in SC.  Instead of relying on exact matches, Soft-SC scores each generated action based on the aggregated probabilities of its constituent tokens.\n",
      "\n",
      "**2.1 Soft Self-Consistency (Soft-SC) Algorithm:**\n",
      "\n",
      "1. **Input:** A task description  `x`.\n",
      "2. **Sampling:** Generate `k` solutions using temperature-based sampling (Ackley et al., 1985; Ficler and Goldberg, 2017), resulting in actions `y₁, y₂, ..., yₖ`. Each `yᵢ` is a sequence of tokens.\n",
      "3. **Scoring:** For each action `yᵢ` composed of tokens `yᵢ₁, yᵢ₂, ..., yᵢₙ`, compute its score using one of the following aggregation functions:\n",
      "    * **Mean:**  `score(yᵢ) = (1/n) * Σᵢ P(yᵢⱼ | yᵢ<ⱼ, x)`\n",
      "    * **Min:** `score(yᵢ) = minᵢ P(yᵢⱼ | yᵢ<ⱼ, x)`\n",
      "    * **Product:** `score(yᵢ) = exp((1/n) * Σᵢ log P(yᵢⱼ | yᵢ<ⱼ, x))`  (where `P(yᵢⱼ | yᵢ<ⱼ, x)` is the model's probability of token `yᵢⱼ` given previous tokens and the input `x`). The best-performing function is chosen based on the development set for each task.\n",
      "4. **Selection:** Choose the action with the highest score:  `ŷ = argmaxⱼ score(yⱼ)`\n",
      "\n",
      "\n",
      "**2.2 Adaptive Soft Self-Consistency:**\n",
      "\n",
      "To enhance efficiency, the authors adapt the idea of adaptive consistency (Aggarwal et al., 2023).  Instead of generating a fixed number of samples `k`, Soft-SC adaptively samples actions until a cumulative score threshold τ is met.  The threshold τ is determined based on development set performance.  Specifically, sampling stops when:\n",
      "\n",
      "`Σⱼ₌₁ᵏ minᵢ P(yⱼᵢ | yⱼ<ᵢ, x) ≥ τ`\n",
      "\n",
      "**2.3 Datasets:**\n",
      "\n",
      "The paper evaluates Soft-SC on three diverse interactive LLM agent datasets:\n",
      "\n",
      "* **Bash:**  (Yang et al., 2023) Involves generating bash commands to fulfill user instructions. Success is measured by the success rate (1.0 reward).\n",
      "* **WebShop:** (Yao et al., 2022) A simulated online shopping environment where the agent interacts with a website to buy products. Performance is measured by success rate (perfect score of 1) and average score (0 to 1).\n",
      "* **ALFWorld:** (Shridhar et al., 2021) A text-based game simulating household tasks where the agent performs a sequence of actions to achieve a goal. Success is measured by success rate.\n",
      "\n",
      "\n",
      "**3. Results and Discussion:**\n",
      "\n",
      "The experiments compare Soft-SC against several baselines: Greedy Decoding (single sample), Self-Consistency (SC), and Adaptive Consistency (AC).  Key findings:\n",
      "\n",
      "\n",
      "* **Superior Performance:** For the same number of samples (`k`), Soft-SC consistently outperforms SC across all datasets.  Improvements range from 1.3% to 6.6% in success rate.\n",
      "* **Improved Sample Efficiency:** Soft-SC achieves comparable or better performance than SC with significantly fewer samples.  Figure 1 visually demonstrates this improved scaling with `k`.\n",
      "* **Better Scaling with Model Size:** Soft-SC shows better scaling with increasing model size than SC (Figure 2).\n",
      "* **Effective with Black-Box Models:** Soft-SC can be applied to black-box models by using a smaller, open-source LLM to score the outputs of the black-box model (Figure 3).  This allows for efficient reranking even without access to the black-box model's logits.\n",
      "* **Calibration Not Crucial:**  The authors find that model calibration (measured by ECE and AUROC) does not strongly correlate with Soft-SC's performance (Appendix B), suggesting its robustness.\n",
      "* **Logit-based Scores Superior:**  Logit-based scores outperform verbalized confidence scores (Table 2).\n",
      "\n",
      "**4.  Limitations and Broader Impacts:**\n",
      "\n",
      "* **Diversity Dependence:** Both SC and Soft-SC require some diversity in the generated samples; identical samples provide no benefit.\n",
      "* **Computational Cost:** Soft-SC, like other sample-and-select methods, incurs a higher computational cost than greedy decoding.\n",
      "* **Ethical Considerations:**  The improved LLM performance could be used for malicious purposes, highlighting the need for responsible development and deployment.\n",
      "\n",
      "**5. Conclusion:**\n",
      "\n",
      "The paper successfully demonstrates that Soft-SC improves both the performance and sample efficiency of LLMs acting as agents in interactive tasks. Its continuous scoring mechanism addresses the limitations of SC in domains with high action diversity and its adaptability to both white-box and black-box models makes it a valuable contribution to the field.\n",
      "\n",
      "\n",
      "**Code Snippets (Conceptual):**\n",
      "\n",
      "The paper doesn't provide specific code snippets but the core logic for Soft-SC's scoring can be represented as follows (Python-like pseudocode):\n",
      "\n",
      "```python\n",
      "def soft_sc_score(action, model, x):\n",
      "  \"\"\"Scores an action based on token probabilities.\"\"\"\n",
      "  tokens = tokenize(action)\n",
      "  probabilities = []\n",
      "  for i, token in enumerate(tokens):\n",
      "    prob = model.probability(token, context=tokens[:i] + x) #Simplified probability function\n",
      "    probabilities.append(prob)\n",
      "  #Choose aggregation function (mean, min, product) based on dataset.\n",
      "  return aggregate(probabilities)\n",
      "\n",
      "def aggregate(probabilities):\n",
      "    #Mean, Min, or Product as defined in the paper.\n",
      "    pass \n",
      "```\n",
      "\n",
      "\n",
      "This summary includes all major aspects of the paper as requested, including algorithms, formulas, datasets, results, discussion, and limitations.  The provided code is a simplified representation,  and the actual implementation details can be found in the GitHub repository linked in the paper.\n",
      "\n",
      "\n",
      "\n",
      "This paper introduces Stepwise Self-Consistent Chain-of-Thought (SSC-CoT), a novel algorithm designed to enhance Large Language Models' (LLMs) ability to perform complex mathematical reasoning.  The core issue addressed is the difficulty LLMs face in handling multi-step reasoning problems, specifically in selecting crucial intermediate steps and exploring solution paths effectively.\n",
      "\n",
      "**1. Methodology:**\n",
      "\n",
      "SSC-CoT employs a multi-step approach inspired by human problem-solving strategies:\n",
      "\n",
      "* **Step 1: Information Extraction:**  The LLM extracts key information (trigonometric functions and angles) from the input question Q using an extraction function  `E(p<sub>θ</sub>, Q)`, where `p<sub>θ</sub>` represents the LLM with parameters θ.\n",
      "\n",
      "* **Step 2: Knowledge Graph Query:** This extracted information is used to query a specifically designed Knowledge Graph (KG)  `r<sub>k</sub> = S(G, V)`, where `S` is the search function, and `G` represents the KG containing relevant trigonometric identities and relationships. The result `r<sub>k</sub>` serves as a \"hint\" for the next step.\n",
      "\n",
      "* **Step 3: Reasoning Chain Generation:** The LLM generates N reasoning chains (`C<sub>i</sub> = G<sub>k</sub>(p<sub>θ</sub>, Q, r<sub>k</sub>)` for round k, where `G<sub>k</sub>` is the chain generation function) based on the question and the hint from the KG. Each chain consists of intermediate results (states) `x<sub>i</sub><sup>j</sup>`.\n",
      "\n",
      "* **Step 4: Overlapping State Identification:**  The algorithm identifies intermediate results that appear across multiple reasoning chains.  This is done by converting intermediate results into TF-IDF vectors and computing cosine similarity.  Results with similarity above a threshold (T=0.999) are considered overlapping. A human-in-the-loop (HITL) option allows human experts to select overlapping states.\n",
      "\n",
      "* **Step 5: Verification:**  A verification function `V(Q, x<sub>i</sub><sup>1</sup>x<sub>i</sub><sup>2</sup>...x<sub>i</sub><sup>j</sup>)` (implemented using the LLM) checks the correctness of the overlapping states. Only verified states (`S<sub>v</sub>`) are retained.\n",
      "\n",
      "* **Step 6: Iteration:** Steps 2-5 are iterated for a predefined number of rounds, using verified states from previous rounds to refine KG queries and guide further reasoning.\n",
      "\n",
      "* **Step 7: Final Result:** The final answer is determined by a majority vote among conclusion statements from the various reasoning chains.\n",
      "\n",
      "**2. Knowledge Graph (KG):**\n",
      "\n",
      "The KG is a directed graph with two types of nodes: \"Conceptual Nodes\" (e.g., sin³x, cos x) and \"Theorem Nodes\" (e.g., cos(π/2) = 0).  Four types of edges represent relationships: Dependency, Derivation, Application, and Identity links.  The KG is queried based on extracted trigonometric functions and angles from the question.  The authors provide a mechanism to expand the KG with new lemmas derived from solved problems.\n",
      "\n",
      "**3. Intermediate Result Selection:**\n",
      "\n",
      "The algorithm prioritizes deeper states within reasoning chains.  When multiple overlapping groups of intermediate results exist, the selection process, formalized in Algorithm 1, utilizes a pairwise comparison based on the depth of states within chains (Equation 1).  If group A can be inferred from group B, B is selected; otherwise, the union of A and B is considered. Algorithm 1 iteratively applies pairwise comparisons to select the optimal group. If no overlap occurs, the final state from each chain is verified.\n",
      "\n",
      "\n",
      "**4. Datasets:**\n",
      "\n",
      "* **TriMaster100:** A new dataset of 100 complex trigonometry problems (senior high school to Mathematical Olympiad level), with solutions broken down into scored intermediate steps.  This allows for a more nuanced evaluation than focusing solely on final answer accuracy. Human performance on a subset of TriMaster100 was also evaluated.\n",
      "\n",
      "* **MATH Level 5:** A subset of the MATH dataset containing the most difficult 1324 questions, used for benchmarking against other methods.\n",
      "\n",
      "\n",
      "**5. Baselines:**\n",
      "\n",
      "The paper compares SSC-CoT against several state-of-the-art methods:\n",
      "\n",
      "* Tree-of-Thought (ToT)\n",
      "* Chain-of-Thought with Self-Consistency (CoT-SC)\n",
      "* LLEMMA (7B and 34B versions)\n",
      "\n",
      "**6. Results:**\n",
      "\n",
      "* **TriMaster100:** SSC-CoT significantly outperforms baselines, achieving a score 34% higher than CoT-SC (the second-best method).  Even without the KG, SSC-CoT's performance is 29% higher than CoT-SC.\n",
      "\n",
      "* **MATH Level 5:** SSC-CoT surpasses the second-best method by 7.2% in accuracy.  Ablation studies demonstrate the effectiveness of both the KG and the intermediate result selection mechanism.  Qualitative analysis showcases SSC-CoT's ability to identify critical intermediate steps more efficiently than ToT, often leading to correct solutions. However, the paper also highlights cases where SSC-CoT makes errors, primarily due to limitations in the LLM's arithmetic capabilities and the verification process.\n",
      "\n",
      "**7. Code and Data:**\n",
      "\n",
      "The code and the TriMaster100 dataset are available at [https://github.com/zhao-zilong/ssc-cot](https://github.com/zhao-zilong/ssc-cot).\n",
      "\n",
      "**8. Conclusion:**\n",
      "\n",
      "SSC-CoT presents a promising approach to improve LLMs' complex mathematical reasoning capabilities.  The use of a KG and a self-consistent chain-of-thought strategy with a verification mechanism leads to superior performance compared to existing methods.  Future work focuses on improving the automatic selection of overlapping intermediate results and the verification process.\n",
      "\n",
      "\n",
      "**Equation 1 (LaTeX):**\n",
      "\n",
      "```latex\n",
      "\\begin{cases}\n",
      "B, & \\text{if } \\forall m \\in M, b_j|_{b_i=m} > a_j|_{a_i=m}, \\\\\n",
      "A, & \\text{if } \\forall m \\in M, a_j|_{a_i=m} > b_j|_{b_i=m}, \\\\\n",
      "A \\cup B, & \\text{otherwise}.\n",
      "\\end{cases}\n",
      "```\n",
      "\n",
      "This equation describes the logic for selecting between two overlapping groups of intermediate results (A and B) based on their positions within reasoning chains.\n",
      "\n",
      "\n",
      "The paper provides extensive experimental results and qualitative analysis to support its claims.  However, the limitations concerning the LLM's arithmetic capabilities and the potential for improvement in the verification process are acknowledged.\n",
      "\n",
      "\n",
      "\n",
      "This paper investigates the self-consistency of large language models (LLMs) when dealing with entity ambiguity.  The authors argue that while LLMs demonstrate impressive performance due to their vast factual knowledge, inconsistencies in their responses, particularly under ambiguity, raise concerns about their trustworthiness.  The core focus is on how LLMs handle ambiguous entities (entities with multiple meanings, e.g., \"Apple\" as a fruit or company).\n",
      "\n",
      "**Methodology:**\n",
      "\n",
      "The study employs a four-part experimental protocol designed to disentangle an LLM's \"knowing\" (awareness of multiple entity readings) from its \"applying knowledge\" (correctly selecting the appropriate reading based on the context).  The methodology uses 49 ambiguous entities, each with at least two interpretations: a specific entity type (animal, fruit, myth, person, location, abstract concept) and a company name (Table 1).  The research questions are:\n",
      "\n",
      "1. **RQ1:** How well do LLMs resolve entity ambiguity?\n",
      "2. **RQ2:** Is the ability to infer the correct entity type biased towards \"preferred readings\"?\n",
      "3. **RQ3:** Can LLMs self-verify their answers?\n",
      "\n",
      "Four studies comprise the evaluation:\n",
      "\n",
      "* **Study 1 (K): Knowledge Verification:**  Assesses whether the LLMs possess knowledge of both interpretations of each ambiguous entity. Prompts like `\"Tell me about <entity type> <entity>\"` are used.  A secondary study (1a) directly asks about ambiguity awareness using prompts like `\"Can <entity> mean anything other than <entity type>? Answer only with Yes or No.\"`\n",
      "\n",
      "* **Study 2 (K + A): Eliciting Preferences:** Determines each LLM's preferred reading for each entity type by giving an underspecified grouping task: `\"Group the following entities according to what they all have in common: <entities>\"`. The model's choice reveals its preferred interpretation.\n",
      "\n",
      "* **Study 3 (K → A): Knowledge to Application:** Evaluates the ability to apply knowledge.  LLMs are prompted with questions requiring the correct entity reading, e.g., `\"Provide the <entity property> for <entity>\"` (ambiguous) and compared to a non-ambiguous baseline with explicit entity type hints, e.g., `\"Provide the <entity property> for <entity type> <entity>\"`.\n",
      "\n",
      "* **Study 4 (A → K): Applying to Knowing:** Tests self-consistency.  Factual information extracted from Study 3 responses is used to create verification prompts, e.g., `\"Does an animal X have <info> speed?\"`.  The focus is on consistency within the model's own responses, not factual accuracy.\n",
      "\n",
      "**Results and Discussion:**\n",
      "\n",
      "* **RQ1:**  While Study 1 confirmed that LLMs \"know\" about multiple readings, Study 3 showed that they struggle to apply this knowledge correctly when given ambiguous prompts, achieving only 85.3% accuracy on average.  Non-ambiguous prompts (with entity type hints) yielded 90.5% accuracy, indicating a significant ambiguity-related performance drop.\n",
      "\n",
      "* **RQ2:**  A strong bias towards preferred readings was observed (Table 2).  Accuracy was 85.4% for preferred readings and only 74.5% for alternative readings in ambiguous prompts. This bias correlates with entity popularity on Wikipedia, suggesting that frequent exposure to a specific meaning in the training data influences the LLM's preference.\n",
      "\n",
      "* **RQ3:**  LLMs demonstrated poor self-verification capabilities (Figure 3).  Even when given the same information shortly after providing it, models frequently failed to confirm the information's correctness.  Further probing with explanatory prompts revealed that models sometimes contradict themselves in the first answer but offer correct information in the subsequent explanation.\n",
      "\n",
      "**Overall, the paper's findings highlight a critical gap in current LLMs:**  They may possess factual knowledge, but they struggle with consistent and reliable application of that knowledge under ambiguous situations, exhibiting bias towards preferred readings and a lack of self-consistency.  The authors conclude that addressing entity ambiguity is crucial for building more trustworthy LLMs.\n",
      "\n",
      "**Limitations:**\n",
      "\n",
      "The study uses a simplified definition of ambiguity (company vs. non-company).  Further research into the varying degrees of polysemy and potential ambiguity in entity properties is suggested.\n",
      "\n",
      "\n",
      "**Code and Data:**\n",
      "\n",
      "The authors provide code and model outputs on GitHub: [https://github.com/anasedova/ToKnow_or_NotToKnow](https://github.com/anasedova/ToKnow_or_NotToKnow)\n",
      "\n",
      "The Appendices provide further detail on entity popularity analysis, annotation procedures, prompt variations, and additional experimental results, including tables showing detailed accuracy breakdown and examples of LLM responses across all studies.  These details are too extensive to fully reproduce here.\n",
      "\n",
      "\n",
      "\n",
      "## Universal Self-Consistency for Large Language Model Generation: An Extensive Summary\n",
      "\n",
      "This paper introduces Universal Self-Consistency (USC), a method designed to improve the quality of Large Language Model (LLM) generated outputs, particularly for open-ended tasks where traditional self-consistency techniques fall short.  The core idea is to leverage the LLM itself as a consistency evaluator, selecting the most consistent answer from multiple candidate responses generated by the same model.\n",
      "\n",
      "**1. Literature Review and Problem Statement:**\n",
      "\n",
      "The paper reviews existing methods for improving LLM outputs, including neural reranking and LLM-based scoring of responses.  It highlights the success of self-consistency with chain-of-thought (CoT) prompting for tasks with structured answers (e.g., single numerical answers to mathematical problems).  However,  standard self-consistency relies on an answer extraction process to aggregate results (typically via majority vote based on exact match), limiting its applicability to free-form text generation tasks (e.g., summarization, open-ended question answering).  The authors argue that assessing consistency is inherently simpler than directly evaluating answer quality, addressing a weakness of existing LLM-based evaluation methods.\n",
      "\n",
      "**2. Methodology: Universal Self-Consistency (USC)**\n",
      "\n",
      "USC addresses the limitations of standard self-consistency by using the LLM to perform the consistency assessment directly. The methodology is as follows:\n",
      "\n",
      "1. **Multiple Response Generation:** The LLM generates multiple responses (`k` responses, typically 8 in the experiments) to the same prompt using a chosen decoding scheme (e.g., temperature > 0).\n",
      "2. **Consistency Prompt:**  All generated responses are concatenated into a single prompt, which includes instructions directing the LLM to select the *most consistent* response from the candidates.  (See Figures 6 and 7 for examples).\n",
      "3. **Response Selection:** The LLM then outputs the index (or identifier) of the selected response.\n",
      "\n",
      "**3. Mathematical Formulation and Notation:**\n",
      "\n",
      "The paper does not introduce any novel mathematical formulas.  The underlying logic is based on the concept of consistency, but it's not formally defined mathematically. The selection process is implicitly defined by the LLM's behavior in response to the consistency prompt.\n",
      "\n",
      "**4. Algorithms:**\n",
      "\n",
      "The core algorithm of USC is straightforward:\n",
      "\n",
      "```python\n",
      "def universal_self_consistency(prompt, model, k=8, temperature=0.6):\n",
      "  \"\"\"\n",
      "  Performs Universal Self-Consistency.\n",
      "\n",
      "  Args:\n",
      "    prompt: The input prompt.\n",
      "    model: The LLM.\n",
      "    k: The number of responses to generate.\n",
      "    temperature: The temperature parameter for sampling.\n",
      "\n",
      "  Returns:\n",
      "    The index of the selected response.\n",
      "  \"\"\"\n",
      "  responses = [model.generate(prompt, temperature=temperature) for _ in range(k)]\n",
      "  combined_prompt = f\"Evaluate these responses:\\n{chr(10).join(responses)}\\nSelect the most consistent response based on majority consensus.\\nStart your answer with 'The most consistent response is Response X' (without quotes).\"\n",
      "  selection = model.generate(combined_prompt) #Assumed to return \"The most consistent response is Response X\"\n",
      "  response_index = int(selection.split(\"Response \")[1][0])\n",
      "  return response_index\n",
      "\n",
      "```\n",
      "\n",
      "**5. Experiments and Results:**\n",
      "\n",
      "The authors evaluate USC on four categories of tasks:\n",
      "\n",
      "* **Mathematical Reasoning:** GSM8K and MATH datasets.  USC achieves comparable performance to standard self-consistency (SC), even without explicit answer extraction. (Table 1)\n",
      "* **Code Generation:** BIRD-SQL (text-to-SQL) and ARCADE (Python code generation) datasets. USC matches the performance of execution-based self-consistency (which requires running the generated code), without needing code execution. (Table 2)\n",
      "* **Long-Context Summarization:** GovReport and SummScreen datasets.  USC significantly outperforms baseline methods (greedy decoding, random selection) on ROUGE and BERTScore metrics. (Table 3)\n",
      "* **Open-Ended Question Answering:** TruthfulQA dataset. USC shows improved truthfulness and informativeness compared to baselines, evaluated using GPT-3 judges. (Table 4)\n",
      "\n",
      "**Tables:**  The paper presents several tables summarizing the experimental results (Tables 1-4, 10, 11). These tables compare USC against greedy decoding, random sampling, standard self-consistency (where applicable), and execution-based self-consistency (where applicable).  Appendix A includes additional tables (Tables 7-9) comparing performance to an oracle (the best possible selection from the candidates).\n",
      "\n",
      "\n",
      "**6. Ablation Studies:**\n",
      "\n",
      "* **Response Ordering:**  USC is robust to the order of responses in the combined prompt. (Table 5)\n",
      "* **Number of Responses (k):** Increasing `k` generally improves performance, but there are diminishing returns and potential downsides related to context length limitations. (Figure 3)\n",
      "* **Selection Criteria:**  Modifying the prompt to select the \"most detailed\" response instead of the \"most consistent\" one can yield further performance gains in summarization tasks. (Table 6)\n",
      "\n",
      "**7. USC vs. Standard Self-Consistency:**\n",
      "\n",
      "The authors analyze the alignment between USC and SC's selections on tasks where both are applicable. They find that a significant portion of discrepancies are due to \"tied votes,\" where multiple responses have the same maximum vote count.  The match rate between USC and SC's choices is often higher than their individual accuracies, suggesting that the consistency criterion is easier to evaluate than correctness. (Figure 4, 5)\n",
      "\n",
      "**8. Conclusion and Limitations:**\n",
      "\n",
      "USC successfully extends self-consistency to free-form generation tasks and matches the performance of standard self-consistency on tasks where it is applicable. However, limitations include context length restrictions on the number of responses and the lack of a built-in confidence measure.  Future work includes addressing these limitations, mitigating position bias, and improving long-context understanding in LLMs.\n",
      "\n",
      "\n",
      "**Overall, the paper presents a novel and practical approach to improving LLM outputs.  USC's simplicity and broad applicability make it a valuable contribution to the field, while the identified limitations suggest avenues for future research.**\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T22:15:23.794061Z",
     "start_time": "2024-11-16T22:15:23.314484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import nbformat as nbf\n",
    "\n",
    "# Define directories\n",
    "synopsis_directory = r\"\"\n",
    "notebook_path = os.path.join(r\"\", \"Literature Review.ipynb\")\n",
    "\n",
    "# Mapping of synopsis filenames to their citations\n",
    "# papers = [\n",
    "#     {\n",
    "#         \"filename\": \"1-s2.0-S0014292117302003-main.md\",\n",
    "#         \"citation\": \"Hellmann, T., & Thiele, V. (2019). Fostering entrepreneurship: Promoting founding or funding?. European Economic Review, 113, 47-68.\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"filename\": \"1-s2.0-S0020025519310904-main.md\",\n",
    "#         \"citation\": \"Zhang, Y., & Zhao, X. (2020). Recommending investors for new startups by integrating network diffusion and investors' domain preference. Information Sciences, 516, 182-196.\"\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "papers = [{\"filename\": f} for f in os.listdir(r\"\")]\n",
    "\n",
    "# Create a new notebook\n",
    "nb = nbf.v4.new_notebook()\n",
    "\n",
    "# Add the main heading\n",
    "main_heading = \"# Literature Review\"\n",
    "nb.cells.append(nbf.v4.new_markdown_cell(main_heading))\n",
    "\n",
    "# Iterate through each paper and add citation and synopsis\n",
    "for idx, paper in enumerate(papers, start=1):\n",
    "    filename = paper[\"filename\"]\n",
    "    citation = \"\"  # paper[\"citation\"]\n",
    "    synopsis_path = os.path.join(synopsis_directory, filename)\n",
    "\n",
    "    # Check if the synopsis file exists\n",
    "    if not os.path.isfile(synopsis_path):\n",
    "        print(f\"Synopsis file not found: {synopsis_path}\")\n",
    "        continue\n",
    "\n",
    "    # Read the synopsis content\n",
    "    with open(synopsis_path, 'r', encoding='utf-8') as f:\n",
    "        synopsis_content = f.read()\n",
    "\n",
    "    # Create citation markdown\n",
    "    citation_md = f\"## {idx}. {citation}\"\n",
    "    \n",
    "    # Add citation and synopsis to the notebook\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(citation_md))\n",
    "    nb.cells.append(nbf.v4.new_markdown_cell(synopsis_content))\n",
    "    print(f\"Added section for: {filename}\")\n",
    "\n",
    "# Write the notebook to a file\n",
    "with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "    nbf.write(nb, f)\n",
    "\n",
    "print(f\"\\nJupyter Notebook 'Literature Review.ipynb' has been created at {notebook_path}\")"
   ],
   "id": "19224b9c41a00640",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added section for: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.md\n",
      "Added section for: DeepSeek-Prover-V1.5 Harnessing Proof Assistant Feedback.md\n",
      "Added section for: DeepSeekMath Pushing the Limits of Mathematical Reasoning in Open Language Models.md\n",
      "Added section for: Integrative Decoding Improve Factuality via Implicit Self-consistency.md\n",
      "Added section for: Measuring Mathematical Problem Solving With the MATH Dataset.md\n",
      "Added section for: OlympiadBench A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems.md\n",
      "Added section for: PEDAL Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars.md\n",
      "Added section for: Plan-and-Solve Prompting Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models.md\n",
      "Added section for: Program of Thoughts Prompting Disentangling Computation from Reasoning for Numerical Reasoning Tasks.md\n",
      "Added section for: Putnam-AXIOM A Functional and Static Benchmark.md\n",
      "Added section for: PutnamBench A Multilingual Competition-Mathematics Benchmark for Formal Theorem-Proving.md\n",
      "Added section for: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.md\n",
      "Added section for: Self-consistency improves chain of thought reasoning in language models.md\n",
      "Added section for: Self-Para-Consistency Improving Reasoning Tasks at Low Cost for Large Language Models.md\n",
      "Added section for: Soft Self-Consistency Improves Language Model Agents.md\n",
      "Added section for: Stepwise Self-Consistent Mathematical Reasoning with Large Language Models.md\n",
      "Added section for: To CoT or not to CoT Chain-of-thought helps mainly on math and symbolic reasoning.md\n",
      "Added section for: To Know or Not To Know Analyzing Self-Consistency of Large Language Models under Ambiguity.md\n",
      "Added section for: Toolformer Language Models Can Teach Themselves to Use Tools.md\n",
      "Added section for: ToRA A Tool-Integrated Reasoning Agent for Mathematical Problem Solving.md\n",
      "Added section for: Training Verifiers to Solve Math Word Problems.md\n",
      "Added section for: Universal Self-Consistency for Large Language Model Generation.md\n",
      "\n",
      "Jupyter Notebook 'Literature Review.ipynb' has been created at C:\\Users\\param\\OneDrive - Neo\\OneDrive\\Documents\\PyCharmProjects\\UCSD-CSE\\CSE 256\\Final-Project\\research-papers\\Literature Review.ipynb\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
