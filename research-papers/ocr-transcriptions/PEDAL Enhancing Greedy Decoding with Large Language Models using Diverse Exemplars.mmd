# PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars

Sumanth Prabhu

sumanth@parsimoai.com

###### Abstract

Self-ensembling techniques with diverse reasoning paths such as Self-Consistency have demonstrated remarkable performance gains in text generation with Large Language Models (LLMs). However, such techniques depend on the availability of an accurate answer extraction process to aggregate across multiple outputs. Moreover, they acquire higher inference cost, in comparison to Greedy Decoding, due to generation of relatively higher number of output tokens. Research has shown that the free form text outputs from Self-Consistency can be aggregated reliably using LLMs to produce the final output. Additionally, recent advancements in LLM inference have demonstrated that usage of diverse exemplars in prompts have the ability to induce diversity in the LLM outputs. Such proven techniques can be easily extended to self-ensembling based approaches to achieve enhanced results in text generation. In this paper, we introduce PEDAL (Prompts based on Exemplar Diversity Aggregated using LLMs), a hybrid self-ensembling approach, that combines the strengths of diverse exemplar based prompts and LLM based aggregation to achieve improvement in overall performance. On the publicly available SVAMP and ARC datasets, our experiments reveal that PEDAL can achieve better accuracy than Greedy Decoding based strategies with lower inference cost compared to Self Consistency based approaches.

## 1 Introduction

Large Language Models (LLMs) Brown et al. (2020); Raffel et al. (2020); Chowdhery et al. (2022); Touvron et al. (2023) have been proven to show remarkable performance in a wide range of Natural Language Understanding tasks Zhao et al. (2023) as a result of their outstanding reasoning (Wei et al., 2022; Zhou et al., 2022). However, they still rely on carefully designed prompts to achieve optimal performance (Khattab et al., 2023; Fernando et al., 2023). To realize further improvement in LLM reasoning, Wang et al. (2022) proposed a self-ensembling technique termed "Self-Consistency"(SC) where diverse "Chain-of-Thought"(CoT) Wei et al. (2022) reasoning paths were generated and then aggregated to construct an accurate and reliable response. This approach has been successfully extended to various use-cases such as LLM hallucination detection (Chen et al., 2024), medicine(Zhou et al., 2024) and code generation (Huang et al., 2024).

While SC based approaches can significantly improve the robustness of LLM outputs, one of their common drawbacks is that they perform best on a fixed answer set (Wang et al., 2022) or rely on training custom aggregation methods to measure consistency across multiple text outputs. To address this, Chen et al. (2023) proposed "Universal Self Consistency"(USC), an extension of SC, that aggregated the text outputs by re-invoking the LLM. Essentially, USC prompted the LLM to select the most consistent response among the different candidate answers generated by SC and demonstrated that it can achieve improved performance. However, this still leaves us with another drawback of SC which is the cost involved in generating the outputs. Concretely, SC involves generating long and diverse reasoning paths which results in a higher number of output tokens compared to Greedy Decoding based approaches. The cost of output token generation with LLMs is typically more than input token processing due to the difference in the number of forward passes (Shazeer, 2019; Chng, 2024) resulting in a higher inference cost with SC.

(Li et al., 2023) experimented with usage of diverse exemplars in the LLM prompts and combined them with diverse reasoning paths in SC to achieve more accurate results in text generation. We observe that if we leverage diverse exemplars with Greedy Decoding for text generation and aggregate the responses as in USC, we achieve better performance than traditional Greedy Decoding interms of accuracy while also achieving lower cost of inference in comparison to SC based approaches.

In this paper, we present a hybrid self-ensembling approach, PEDAL(**P**rompts based on **E**xemplar **D**iversity **A**ggregated using an **LLM), that offers a trade-off between the Greedy Decoding and SC in terms of accuracy and cost efficiency. We leverage diverse exemplars in LLM prompts to generate multiple candidate responses using Greedy Decoding and then aggregate them using an LLM to generate the final response. On two publicly available datasets, we demonstrate that PEDAL achieves better accuracy than Greedy Decoding based strategies and offers lower cost in inference compared to SC based strategies.

Rest of the paper is organized as follows: In Section 2, we describe previous work for solving similar problems. Section 3 explains our proposed strategy in detail followed by Section 4 where we describe the data and the experiment settings to validate PEDAL. We then present our results and analyses in Section 5. Finally, in Section 6, we summarize our findings and discuss potential future work.

## 2 Related Work

LLMs have been widely studied and applied in a variety of tasks including code generation Zheng et al. (2024), finance Li et al. (2024), law Yu et al. (2022) and so on. However, none of the LLMs seem to consistently outperform the rest of the models across all tasks Jiang et al. (2023). This led to exploring ensembling approaches with LLMs. Research focused on Prompt Chase (2022), Fusion Li et al. (2023), Mixture of Experts Cai et al. (2024) and many more have shown promising results in combining LLMs to enhance the overall performance.

### Self Ensembling Strategies

Long (2023); Yao et al. (2023) generalized CoT to organize language model generated "thoughts" into a tree structure for solution search. However, similar to Wang et al. (2022), they rely on custom aggregation methods to construct the final output. Chen et al. (2023) addressed this issue by leveraging LLMs to perform majority consensus based aggregation without any specific model fine-tuning. In our work, we leverage a similar strategy to aggregate multiple candidates with a focus on the impact of using diverse LLM prompts as opposed to diverse reasoning paths.

### Prompt Ensembling Strategies

With the advent of LLMs, lot of research focused on developing effective prompting techniques Bach et al. (2022); Lu et al. (2022) that have been extended by multiple prompt ensembling techniques Zhang et al. (2023); Pitis et al. (2023) to achieve further improvement. Singh et al. (2023) built a decision tree of prompts that links multiple LM calls to solve a task. Arora et al. (2022) used multiple prompt templates to reformat few-shot example inputs into an open ended question-answering format and then leverage Weak Supervision Ratner et al. (2017) to aggregate the LLM predictions. Hou et al. (2023) applied AdaBoost Schapire (2013) algorithm over a pre-defined prompt set for text classification by pairing prompts with the corresponding output distribution to construct a large pool of weak learners. Li et al. (2023) enhanced SC with diverse prompts by randomly selecting different exemplars for prompt construction, followed by sampling reasoning paths for each such prompt and then scoring the quality of each reasoning path using a custom trained model. While our work also leverages a similar prompt construction strategy, we aggregate the predictions without relying on explicitly training a task-specific model. Additionally, we focus on leveraging such prompt based strategies to reduce LLM inference cost rather than enhancing SC based approaches.

### LLM Inference Cost

To solve the problem of inference cost, researchers have commonly explored model compression techniques Zhu et al. (2024) such as model quantization Jacob et al. (2018), model pruning Cheng et al. (2024) and model distillation Gou et al. (2021) aimed at reducing the size of the model without hurting the performance significantly. Shazeer (2019) proposed sharing keys and values across all of the different attention heads in the transformer architecture, thus, reducing the memory bandwidth requirements of incremental decoding. Wu et al. (2024) explored decoding multiple successive tokens simultaneously in a single forward pass to reduce the inference time. FrugalGPT Chen et al. (2023) proposed a cascade of LMs that stops when an intermediate output is considered reliable, resulting in better computational efficiency. In our work, we focus on reducing the number of output tokens during LLM inference in comparison to SC while achieving better accuracy than Greedy Decoding.

## 3 Methodology

Figure 1 shows the high level overview of our proposed system. The LLM generates multiple candidate responses using Greedy Decoding with prompts based on diverse exemplars. The candidate responses are then aggregated using the same LLM to generate the final output.

### Prompts with Diverse Exemplars

Traditional CoT based approaches rely on a single prompt comprised of a fixed set of exemplars. Li et al. (2023) showed that constructing multiple prompts, by modifying the exemplars chosen for the purpose of In-Context-Learning (ICL), further enhances the reasoning capability of language models. On similar lines, we construct multiple LLM prompts by randomly sampling the exemplars for ICL multiple times using different seed settings. For each such LLM prompt, we generate a candidate response using Greedy Decoding.

### LLM-based Aggregation

USC Chen et al. (2023) that has been shown to accurately select the most consistent response among multiple SC responses using majority consensus. We follow USC and extract the final response from multiple candidate responses accordingly.

## 4 Experiments

### Dataset

We consider two publicly available datasets for the purpose of our experiments -

* **SVAMP**Patel et al. (2021) Comprises of elementary-level Math Word Problems. Each problem consists of a short natural language narrative that describes a state of the world and poses a question about some unknown quantities.
* 'ARC-Easy' and 'ARC-Challenge' where 'ARC-Challenge' partition contains relatively more difficult questions that require reasoning

We report results on the validation split of each dataset. We restrict the ARC dataset to 'ARC-Challenge' only and work with 30% of the data sampled at random. Table 1 captures the corresponding details of the validation datasets considered for the experiments in the paper.

### Baseline Strategies

To benchmark our approach, PEDAL, we include the following baselines

* We run the LLM to select the token with the highest probability at each step to generate the final output.
* We run SC with CoT prompting and select the most consistent answer among all candidate responses using the same LLM.
* To understand the impact of multiple candidate responses generated in PEDAL using diverse prompts, we combine all such diverse exemplars directly into a single ICL prompt and run Greedy Decoding. We refer to this baseline as "Unified Diverse Exemplars" (UDE).

### Experiment Setting

Each of the strategies were run using Qwen2-7B-Instruct Yang et al. (2024) and Llama-3-8B-Instruct Touvron et al. (2023). We measure the performance using accuracy and the number of

\begin{table}
\begin{tabular}{|p{85.4pt}|p{284.5pt}|} \hline
**Dataset Name** & **Number of Validation Samples** \\ \hline SVAMP & 300 \\ \hline ARC & 345 \\ \hline \end{tabular}
\end{table}
Table 1: Validation dataset size for SVAMP and ARC datasets

Figure 1: High level overview of PEDAL (**P**rompts based on **E**xemplar **D**iversity **A**ggregated using an **LLM)

output tokens. For purposes of reporting, we also share the number of input tokens consumed by the strategies. The LLMs were run using 4-bit quantization [2]. Each experiment is run under three random seed settings for reproducibility. We pick three exemplars per experiment for the ICL prompt construction with each dataset. For each experiment, USC is run to generate three intermediate outputs and PEDAL is run with three diverse input prompts.

## 5 Results and Analysis

Table 2 and Table 3 show the performance metrics for different strategies using SVAMP dataset. Similarly, Table 4 and Table 5 capture the performance metrics for the ARC dataset. We observe that our proposed approach consistently performs better than Greedy Decoding in terms of accuracy and outperforms USC in terms of the number of output tokens.

### Arithmetic Reasoning

As shown in Table 2, PEDAL displays improvement over Greedy Decoding on the SVAMP dataset. With Qwen2, PEDAL achieves an average accuracy of 77.89% while Greedy Decoding achieves an average accuracy of 76% implying a 1.89% improvement. PEDAL also outperforms UDE which achieves an accuracy of 75.67%. USC achieves the accuracy of 80.33%. Similarly, with Llama3, we observe that PEDAL achieves an average accuracy of 74.11% while Greedy Decoding achieves a score of 70.22% resulting in 3.89% improvement. However, with Llama3, we observe that USC achieves an accuracy of 72.99% which is lesser than PEDAL while UDE achieves an accuracy 70.67% marginally outperforming Greedy Decoding.

As shown in Table 3, with Qwen2, USC processes approximately 903 input tokens and 503 output tokens while PEDAL processes 1,343 input tokens with 192 output tokens making our approach evidently more cost efficient. With Llama3, USC processes an average of 694 input tokens and 924 output tokens while PEDAL processes 1,262 input tokens and 198 output tokens. While USC relies on lesser input tokens than PEDAL, the cost of output tokens with USC is more than 4 times the output token cost with PEDAL making our approach more cost efficient.

### Multiple-Choice Question Answering

As shown in Table 4, the strategies show a similar relationship with experiments run on the ARC

\begin{table}
\begin{tabular}{|l|l|l|} \hline
**Model** & **Approach** & **Accuracy** \\ \hline \multirow{3}{*}{Qwen2} & Greedy & 83.38 \(\pm\) 0.55 \\ \cline{2-3}  & **USC** & **84.35 \(\pm\) 0.62** \\ \cline{2-3}  & UDE & 84.06 \(\pm\) 0.0 \\ \cline{2-3}  & PEDAL & 83.77 \(\pm\) 0.47 \\ \hline \multirow{3}{*}{Llama3} & Greedy & 76.52 \(\pm\) 1.44 \\ \cline{2-3}  & USC & 71.88 \(\pm\) 0.71 \\ \cline{2-3}  & UDE & 76.52 \(\pm\) 0.0 \\ \cline{2-3}  & **PEDAL** & **78.55 \(\pm\) 0.47** \\ \hline \end{tabular}
\end{table}
Table 4: Performance comparison of greedy decoding, USC, UDE and PEDAL for ARC dataset using Accuracy. Averaged scores across 3 seeds are reported along with the standard deviation. Best performing strategy per model has been highlighted in **bold**

\begin{table}
\begin{tabular}{|l|l|l|} \hline
**Model** & **Approach** & **Accuracy** \\ \hline \multirow{3}{*}{Qwen2} & Greedy & 76.0 \(\pm\) 1.52 \\ \cline{2-3}  & **USC** & **80.33 \(\pm\) 0.98** \\ \cline{2-3}  & UDE & 75.67 \(\pm\) 0.0 \\ \cline{2-3}  & PEDAL & 77.89 \(\pm\) 1.28 \\ \hline \multirow{3}{*}{Llama3} & Greedy & 70.22 \(\pm\) 1.03 \\ \cline{2-3}  & USC & 72.99 \(\pm\) 0.47 \\ \cline{2-3}  & UDE & 70.67 \(\pm\) 0.0 \\ \cline{2-3}  & **PEDAL** & **74.11 \(\pm\) 0.57** \\ \hline \end{tabular}
\end{table}
Table 2: Performance comparison of Greedy Decoding, USC, UDE and PEDAL for SVAMP dataset using Accuracy. Averaged scores across 3 seeds are reported along with the standard deviation. Best performing strategy per model has been highlighted in **bold**

\begin{table}
\begin{tabular}{|l|l|l|} \hline
**Model** & **Approach** & **Accuracy** \\ \hline \multirow{3}{*}{Qwen2} & Greedy & 76.0 \(\pm\) 1.52 \\ \cline{2-3}  & **USC** & **80.33 \(\pm\) 0.98** \\ \cline{2-3}  & UDE & 75.67 \(\pm\) 0.0 \\ \cline{2-3}  & PEDAL & 77.89 \(\pm\) 1.28 \\ \hline \multirow{3}{*}{Llama3} & Greedy & 70.22 \(\pm\) 1.03 \\ \cline{2-3}  & USC & 72.99 \(\pm\) 0.47 \\ \cline{2-3}  & UDE & 70.67 \(\pm\) 0.0 \\ \cline{1-1} \cline{2-3}  & **PEDAL** & **74.11 \(\pm\) 0.57** \\ \hline \end{tabular}
\end{table}
Table 3: Performance comparison of USC and PEDAL for SVAMP dataset using the number of output tokens. Averaged counts across 3 seeds are reported along with the standard deviation. Best performing strategy per model has been highlighted in **bold**dataset. With Qwen2, PEDAL achieves a marginal improvement of 0.39% over Greedy Decoding with an average accuracy of 83.77% while Greedy Decoding has an average accuracy of 83.38%. UDE outperforms PEDAL with an accuracy of 84.06% while USC still achieves the best performance with an accuracy of 84.35%. With Llama-3, PEDAL shows a 2.03% improvement with a score of 78.55% and greedy decoding achieves 76.52%. UDE achieves an accuracy of 76.52% matching the performance of Greedy Decoding. Surprisingly, USC achieves an accuracy of 71.88% which is relatively the least among the strategies. With USC, the main goal of the paper is to benchmark the proposed approach in terms of token count. To prevent diverging from the primary focus area, we leave deeper analysis of this behaviour to future work.

As shown in Table 5, with Qwen2, our approach outperforms USC where USC processes roughly 1,154 input tokens and 669 output tokens on an average while PEDAL processes 1,180 input tokens with 100 output tokens. With Llama3, USC processes 1,073 input tokens and 929 output tokens while PEDAL processes 1,186 input tokens and 197 output tokens. Our approach is the better choice in terms of the number of output tokens processed by the LLM.

### Comparison to CoT

Similar to PEDAL, CoT has been shown to be more accurate than Greedy Decoding and less expensive in terms of inference compared to SC. Based on pre-liminary interpolation of the number of output tokens using Table 3 and Table 5, we compare the number of output tokens consumed in a single intermediate output in SC (equivalent to CoT) with the number of output tokens in PEDAL. With Llama3, we observe that PEDAL would be more cost efficient for both datasets. With Qwen2, we observe that PEDAL would be more cost efficient for the ARC dataset but may prove to be more expensive for the SVAMP dataset in comparison to CoT. While PEDAL seems to be more reliably consistent, it would be interesting to further investigate and arrive at definitive conclusions. We intend to evaluate the merits and drawbacks of both approaches in a practical setting in future work.

### Impact of Number of Diverse Prompts

We re-run the experiments for both datasets with our best performing model, Qwen2, by varying the number of prompts to study how it affects the performance. As shown in Table 6, we additionally run the experiments for two and four diverse prompts under three seed settings. We observe slight improvements as we increase the number of prompts with the SVAMP dataset. However, we do not observe any such specific pattern with the ARC dataset.

## 6 Conclusion

In this paper, we explored self-ensembling with LLMs using diverse exemplars with LLM based output aggregation. We observed that this combination can perform better than Greedy Decoding in terms of accuracy and achieve better cost efficiency than SC based methods. However, we restricted the experiments to small datasets that allowed benchmarking approaches using exact match without additional manual annotation efforts. In future work, we plan to explore possibilities on extending such ensembling strategies to a wider range of problem settings involving free-form text generation to further deep dive into strengths and weaknesses of our proposed system.

\begin{table}
\begin{tabular}{|l|l|l|} \hline
**Number** & **SVAMP** & **ARC** \\
**of** & & \\
**Prompts** & & \\ \hline
2 & 77.0 \(\pm\) 0.98 & 83.96 \(\pm\) 0.36 \\ \hline
3 & 77.89 \(\pm\) 1.28 & 83.77 \(\pm\) 0.47 \\ \hline
4 & 78.22 \(\pm\) 1.34 & 83.87 \(\pm\) 0.49 \\ \hline \end{tabular}
\end{table}
Table 6: Effect of number of prompts on performance using Qwen2 with SVAMP and ARC datasets. Averaged scores across 3 seeds are reported along with the standard deviation.

\begin{table}
\begin{tabular}{|l|l|l|l|} \hline
**Model** & **Approach** & \multicolumn{2}{l|}{**Token Count**} \\ \cline{3-4}  & & **Input** & **Output** \\ \hline \multirow{2}{*}{Qwen2} & USC & 1153.04 & 668.71 \(\pm\) \\  & & \(\pm\) 1.96 & 7.19 \\ \cline{2-4}  & **PEDAL** & 1179.76 & **99.47**\(\pm\) \\  & & \(\pm\) 100.10 & **10.05** \\ \hline \multirow{2}{*}{Llama3} & USC & 1072.96 & 928.1 \(\pm\) \\  & & \(\pm\) 5.67 & 1.31 \\ \cline{2-4}  & **PEDAL** & 1185.27 & **196.83**\(\pm\) \\  & & \(\pm\) 115.08 & **0.11** \\ \hline \end{tabular}
\end{table}
Table 5: Performance comparison of USC and PEDAL for ARC dataset using the number of output tokens. Averaged counts across 3 seeds are reported along with the standard deviation. Best performing strategy per model has been highlighted in **bold**

## References

* Arora et al. (2022) Simran Arora, Avanika Narayan, Mayee F. Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher Re. 2022. Ask me anything: A simple strategy for prompting language models.
* Bach et al. (2022) Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Alsaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. 2022. Prompt-Source: An integrated development environment and repository for natural language prompts. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_, pages 93-104, Dublin, Ireland. Association for Computational Linguistics.
* Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS'20, Red Hook, NY, USA. Curran Associates Inc.
* Cai et al. (2024) Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts.
* Chase (2022) Harrison Chase. 2022. LangChain.
* Chen et al. (2024) Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2024. Inside: Llms' internal states retain the power of hallucination detection.
* Chen et al. (2023a) Lingjiao Chen, Matei Zaharia, and James Zou. 2023a. Frugalgpt: How to use large language models while reducing cost and improving performance.
* Chen et al. (2023b) Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023b. Universal self-consistency for large language model generation. _ArXiv_, abs/2311.17311.
* Cheng et al. (2024) Hongrong Cheng, Miao Zhang, and Javen Qinfeng Shi. 2024. A survey on deep neural network pruning-taxonomy, comparison, analysis, and recommendations.
* Chng (2024) Peter Chng. 2024. Why do llm input tokens cost less than output tokens?
* Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsyvashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunjia Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Eric Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstrom, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. _J. Mach. Learn. Res._, 24:240:1-240:113.
* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge.
* Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms.
* Fernando et al. (2023) Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktaschel. 2023. Promptbreeder: Self-referential self-improvement via prompt evolution.
* Gou et al. (2021) Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. _International Journal of Computer Vision_, 129(6):1789-1819.
* Hou et al. (2023) Bairu Hou, Joe O'Connor, Jacob Andreas, Shiyu Chang, and Yang Zhang. 2023. Promptboosting: black-box text classification with ten forward passes. In _Proceedings of the 40th International Conference on Machine Learning_, ICML'23. JMLR.org.
* Huang et al. (2024) Baizhou Huang, Shuai Lu, Weizhu Chen, Xiaojun Wan, and Nan Duan. 2024. Enhancing large language models in coding through multi-perspective self-consistency.
* Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Jiang et al. (2023) Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion.

Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines.
* Li et al. (2023a) Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, and Li Shen. 2023a. Deep model fusion: A survey.
* Li et al. (2023b) Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023b. Making language models better reasoners with step-aware verifier. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5315-5333, Toronto, Canada. Association for Computational Linguistics.
* Li et al. (2024) Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2024. Large language models in finance: A survey.
* Long (2023) Jieyi Long. 2023. Large language model guided tree-of-thought.
* Lu et al. (2022) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8086-8098, Dublin, Ireland. Association for Computational Linguistics.
* Patel et al. (2021) Arkil Patel, Satwik Bhattacharya, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2080-2094, Online. Association for Computational Linguistics.
* Pitis et al. (2023) Silviu Pitis, Michael R. Zhang, Andrew Wang, and Jimmy Ba. 2023. Boosted prompt ensembles for large language models.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21(1).
* Ratner et al. (2017) Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Re. 2017. Snorkel: rapid training data creation with weak supervision. _Proc. VLDB Endow._, 11(3):269-282.
* Schapire (2013) Robert E Schapire. 2013. Explaining adaboost. In _Empirical inference_, pages 37-52. Springer.
* Shazeer (2019) Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need.
* Singh et al. (2023) Chandan Singh, John Morris, Alexander Rush, Jianfeng Gao, and Yuntian Deng. 2023. Tree prompting: Efficient task adaptation without fine-tuning. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 6253-6267, Singapore. Association for Computational Linguistics.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. _ArXiv_, abs/2302.13971.
* Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. _ArXiv_, abs/2203.11171.
* Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. _ArXiv_, abs/2201.11903.
* Wu et al. (2024) Pengfei Wu, Jiahao Liu, Zhuocheng Gong, Qifan Wang, Jinpeng Li, Jingang Wang, Xunliang Cai, and Dongyan Zhao. 2024. Parallel decoding via hidden transfer for lossless large language model acceleration.
* Yang et al. (2021) An Yang, Baosong Yang, Bingyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haqran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jianxin Yao, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 technical report.
* Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models.
* Yu et al. (2022) Fangyi Yu, Lee Quartey, and Frank Schilder. 2022. Legal prompting: Teaching a language model to think like a lawyer.
* Zhang et al. (2023) Chenrui Zhang, Lin Liu, Jinpeng Wang, Chuyuan Wang, Xiao Sun, Hongyu Wang, and Mingchen Cai. 2023. Prefer: Prompt ensemble learning via feedback-reflect-refine.

Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models.
* Zheng et al. (2024) Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen. 2024. A survey of large language models for code: Evolution, benchmarking, and future trends.
* Zhou et al. (2022) Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Huai hsin Chi. 2022. Least-to-most prompting enables complex reasoning in large language models. _ArXiv_, abs/2205.10625.
* Zhou et al. (2024) Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jingge Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Chenyu You, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo, and David A. Clifton. 2024. A survey of large language models in medicine: Progress, application, and challenge.
* Zhu et al. (2024) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2024. A survey on model compression for large language models.